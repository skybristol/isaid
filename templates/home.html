{% extends "bootstrap/base.html" %}
{% block title %}iSAID Ugly-by-Design Interface{% endblock %}

{% block navbar %}
{{nav.isaid_navbar.render()}}
{% endblock %}

{% block content %}
<blockquote class="blockquote text-left">
    <p class="mb-0">This web app provides a simple interface to view information integrated from multiple sources into an underlying graph database and then synthesized into a faceted search capability. iSAID (codename that stands for the "integrated Science Assessment Information Database") is an experimental toolset that continuously assembles information about people, facilities, and scientific assets useful in assessing overall capacity to carry out integrated science. The interface is deliberately ugly by design as it is intended to provide a full dive into the data and not a specific type of optimized presentation.</p>

    <p class="mb-0">The faceted search, which is the main point of this web app, is now built as a synthesis from an underlying graph database discussed and executed through code in a set of <a href="https://github.com/skybristol/pylinkedcmd/isaid">Jupyter Notebooks</a>. The graph exposes relationships between assets that let us derive properties for those assets based on inherent or derived linkages and shared attributes. For instance, a person may not be described explicitly in something like their USGS Profile Page with certain terms, but we know about their publications and datasets through catalog connections and can infer that a person has addressed certain subject matter in their work from those other assets. We expose these as search facets to help round out what we know about a person.</p>

    <p class="mb-0">I'm also actively working on improving the quality and usability of all of the underlying data pulled together for this exercise. Most of this has to do with semantic alignment of various concepts and assertions in the metadata about assets or their actual digital content in some cases. I'm essentially building a large reference vocabulary, partly comprised of the various vocabularies pulled together for the USGS Thesaurus work, but extending beyond that to sources from Wikidata that are sufficiently documented and referenced to be usable and other specialized vocabularies or glossaries with concepts we want to pull our of our data. I run some rules-based entity recognition against the reference source for cases where we nominally have reasonable keyword lists, including some that explicitly indicate a particular reference source. The results of this show that we have a lot of messy keywords that cannot be reasonably tied to anything, and these go into the graph as UndefinedSubjectMatter terms. When we can establish a reasonable match to appropriate vocabulary references, those go into DefinedSubjectMatter terms and are the main source behind the facets in the search tool here.</p>

    <p class="mb-0">In addition to processing keywords, I'm also working with an entity linking and recognition process using natural language processing and a machine-learning approach. This is an iterative process of building training data for both NER models and Entity-Linking models, running those against all descriptive texts and undefined subject matters assembled for entities, validating back to the reference source, and improving the models with tests. This is still very much a work in progress part of the research in this project, but I'm slowly feeding new DefinedSubjectMatter terms into the graph as I'm able to run models effectively for concepts that improve our ability to characterize assets.</p>
    
    <h3>Things you can do here</h3>
    <ul>
        <li>Explore a crude <a href="/search">faceted search interface</a> for the various types of entities we are bringing together (click on entities to view full details available in the indexes)</li>
        <li>Read about work in progress for this constantly evolving project through working notes in a concept-map type of interface from "The Brain" availble online <a href="https://bra.in/3jMBbe">here</a></li>
        <li>Visit the underlying Python code used to build data - <a href="https://github.com/skybristol/pylinkedcmd">pyLinkedCMD</a></li>
        <li>Read about how the iSAID graph is built - <a href="https://github.com/skybristol/pylinkedcmd/isaid">pyLinkedCMD iSAID notebooks</a></li>
    </ul>

    <h3>Data Model</h3>
    <p class="mb-0">iSAID explores methods needed to incorporate data about people, organizations, and other scientific assets (publications, data, models, tools, instruments, etc.) and the relationships between all of these "end members" for various purposes, including a faceted search index. In our current environment, the metadata and other documentation about these items is scattered throughout many different information stores with a whole raft of underlying models and assumptions that are not always clearly documented. For this reason, we are attempting to define and iteratively build out an abstract data model and populate it with code-driven processes that can continually operate against available sources. All sources are public at this time, but we will be exploring means of synthesizing useful, publicly accessible information from certain internal sources that currently lack reasonable interfaces but where content can be made accessible to help further characterize scientific assets.</p>
    <p class="mb-0">Our data model is generally informed from two main sources of inspiration:</p>
    <ul>
        <li>Wikidata's model of items (entities) with claims (statements) that are referenced and qualified. This closely resembles the original ScienceBase model as well where we took a similar "everything is an item" approach. A key tenet of this model is that every claim should be made using a property from a limited set that are all well defined in structure and semantic definition. Another key tenet is that end members to which a claim refers should also be defined, linkable entities in our system (or broadly on the web). Also, every claim should be referenced as to its provenance (the more explicit and detailed the better), and qualifiers should at least include a date that the claim was known to be valid.</li>
        <li>A core set of properties for all entities is inspired by schema.org schemas as generally simplified models for things that we need to describe and bring together in iSAID. Part of our goal is to promote the implementation of linked open data methods and way of thinking across all of the systems we are tapping for this work. If all the underlying information platforms could implement the simple concepts of accessible (e.g., not overly complicated with fancy whiz bang HTML rendering techniques) landing pages that advertise their primary assets and incorporated linked data methods of embedding structured information according to community-accepted specifications, then most of the complicated procedures we have had to write into the pyLinkedCMD package (discussed below) would be unnecessary.</li>
    </ul>
    <p class="mb-0">Although all of the information we are bringing together for this exercise could be organized into a very low level triple store data model, working with data in that construct is not as intuitive for the average user. We've opted to develop and use a slight abstraction from that in a basic linked document model, focusing on people and organizations, scientific assets (other than people), and statements/claims derived about all of those. We put a very small number of properties into our directory of people and organizations and the highest level summary information on assets, all based on schema.org (people, orgs, and creative works so far). This is mostly considered demographic or basic descriptive and locational information about those entities, and claims are focused on the interesting bits that we want to analyze, harmonize, and leverage in assessment.</p>

    <h4>Data Processing Logic</h4>

    <p class="mb-0">We are also exploring the use of completely independent and asynchronous processing using serverless methods on the cloud (message queues and lambdas). This forces a particular microservices mindset where everything that happens in our workflow needs to be able to operate independently in very small steps that trigger other steps when conditions warrant. It also helps us solve a few information management problems along the way. The following are the logical steps we are taking for each data/information source we are processing:</p>

    <ol>
        <li><strong>Source Data Gathering</strong> - Source gathering operates on variable schedules based on when things are likely to change at a source or when our system "demands" to be updated. In some cases, the source gathering lambda may be able to check a source for changes that have occurred since some previous date/time and then operate on those records. In other cases, we will have some other type of interface that will let us check a date stamp on individual documents against what we have in our cache. In still other cases, we will need to retrieve an entire document and check it against what we have in our cache to check for differences.</li>
        <li><strong>Source Caching</strong> - Data gathering lambdas store original source documents in a cache. For now, we are getting everything from source as JSON documents, sometimes with a basic transformation to get everything into a personal preference digestible structure. This means that we can cache everything into Elasticsearch, allowing for efficient processing and some built in versioning features. Caching allows us to run processing against the cached documents completely independently without having to go back to sources that can sometimes be flaky. It also gives us versioning capability, something that we often lack from original source systems.</li>
        <li><strong>Graphing the entire USGS</strong> - I've recently shifted to jumping straight into graphing everything we bring together from source material into a set of entities and relationships. In essence, we'll be attempting to build one giant knowledge graph of everything we can connect through all the threads we are weaving together. This follows the same logic of entities with claims, mostly placing the references and date qualifiers on the relationships between nodes in the graph. The graph is then used to build the facets on entities for our search.</li>
        <li><strong>Making Claims</strong> - Each source can be digested into sometimes many different claims that link subjects to objects via a property. Claims from individual sources are inherently unique as they come from a particular reference though they may represent the same logical information about subject and object and be compounded later. The claims creation process will create variable information for later use, and we will continue refining this through use case application until we get it right. Claims are often made in a reciprocal manner from subject to object and back through a different property label (e.g., "funder of" and "funded by").</li>
        <li><strong>Entity Identification</strong> - The claims creation process interprets the information we get from sources to specify subjects and objects, bringing into focus any potentially unique and hopefully persistent and resolvable identifiers, labels, and an interpretation or judgment call on the type of thing each subject/object represents. Based on this "instance_of" property, we have an opportunity to create and build upon unique entity records, which are really the heart of our model. Each new claim means that we have either one or two new entities or have brought in additional information that can be added to existing entities. The best cases are where we have some type of reasonably persistent identifier (e.g., DOIs and ORCIDs) that we are pretty sure we can count on. Those types of identifiers for subject/objects give us an anolity to resolve them through content negotiation from a registry to a) make sure we have valid identifiers and b) run other checks as necessary to see if the information in our claims checks out. Once we've determined a new entity that showed up as subject/object to be actionable, we can start stubbing out a record. Over time, we will end up with some number of "unverified claims" in our system that we can routinely send to a message queue for processing to determine if something new has come up.</li>
        <li><strong>Claims Refinement</strong> - Certain claims lend themselves to additional work that we can kick off to make more usable information for various purposes. For instance, we often get many different and sometimes quite messy string representations of geographic places (where people work, where scientists study processes, etc.). We can refine these with geolocation techniques to harmonize the data down to something more useful (e.g., places that can be plotted on a map). Claims refinement takes a given source claim as its starting point, does some work on the information, and generates a new claim that is referenced to the algorithm that generated it. Our system (or users) can then decide what claims they want to use for a particular purpose.</li>
        <li><strong>Entity Strengthening</strong> - Over time, every claim that can be linked to an identified entity in our model strengthens our understanding of those entities and our ability to use the information for discovery and analysis. Claims digestion into entities adds additional values to facilitate discovery (search, faceting, etc.) and analysis. We're experimenting with a combination of basic search functionality enhanced by adding additional discoverable information to entity documents, raw facets from original source terms, and enhanced facets from refinement operations, providing all of these in an index for application to discovery use cases.</li>
    </ol>

    <h4>Facets and Enhancers</h4>
    <p class="mb-0">The process of assembling everything as claims about identified entities is allowing iSAID processing an easy way of crawling through various information sources, making the best sense of them structurally as we can, and then storing a simple data structure for further use. But, at the end of the day, we need to use those claims to good effect to better characterize our entities in various ways so that we can make them more discoverable, develop logical linkages/relationships between entities based on the significance of those characteristics, and exploit that improved characterization for various analytical purposes such as assessing capacity to do certain things and identifying gaps. To this end, we've started experimenting with leveraging the power and functionality of faceted search engines to tee up the concepts for use in search and filtering results.</p>

    <p class="mb-0">The raw take from our various claim sources is proving to be all together pretty messy and disjointed. Even in sources of supposedly high quality metadata, we find lots of stuff that's ugly to deal with like misspellings and lack of any obvious, explicit reference to authoritative or accepted sources. A lot of our stuff comes from what are uncontrolled text fields in various applications or information sources, meaning that there's been no validation to deal with misspellings and other nuisances. The structure we're using tells us where everything comes from so we have context to work from, but if we want to make best use of this information, we need to work on semantic alignment of the data toward something more robust.</p>

    <p class="mb-0">There has been lots of work on the idea of metadata enhancement pipelines. As soon as you start to aggregate heterogeneous sources of metadata of any kind together, you quickly expose all the inconsistencies and sticky issues that make the catalogs or indexes more difficult to use. The <a href="http://cinergi.sdsc.edu/about/">CINERGI</a> project via NSF EarthCube is one that I collaborated on for a time where the group at the San Diego Supercomputer Center built a catalog for geoscience data of all kinds. They built an enhancement pipeline concept and put some good energy into building a provenance trace using the W3C-PROV specification so that end users could be aware of all the things that happened to data along the way. I've also looked at the <a href="https://magda.io/">MAGDA</a> catalog project recently. It seems to hold some real progress but isn't quite ready for primetime yet and includes a pretty complex architecture.</p>

    <p class="mb-0">What I've long wanted to develop is a more modular approach to the whole notion of metadata/data aggregation and cataloging with the idea of lots of individual enhancers with different configurations based on varying business rules and requirements for what needs to happen with the data. I'm especially interested in the idea of "enhancement modules/services" that can operate on multiple platforms and in different infrastructure circumstances. To that end, I'm starting to work on a processing framework that will critically evaluate claims, do some work to make sense of values, and then feed new values into the system. Sticking with the claims model, I'm having the enhancers make their own claims sourced from original data and referenced to the algorithms that assert a new value. We can then make deliberate choices about what claims we want to trust and use for particular purposes.</p>

    <p class="mb-0">For immediate purposes, I'm working on processing concepts from claims into key characteristics that we can use as search facets. For information elements that are reasonably well structured and understood like locations/places and organizational names, I'm working up reasonable methods for aligning these with appropriate sources. For topical terms, we're focusing in on use of the USGS Thesaurus as much as we can. While it is not a comprehensive or fully robust source, it does represent the best collection of concepts important to describing and organizing USGS science that we have and has been explicitly identified as term source in a few cases. Beyond those areas, we have a tremendous amount of information flowing into the iSAID orbit that is fully unstructured loose form text. Those do represent a really valuable pool of information, however, and I'm exploring options with natural language processing methods from named entity recognition to clustering and other processes to determine utility. I'm working to seed those with appropriate training data and established sources and taking a preponderance of evidence approach where algorithm-derived assertions build on more structured sources where possible.</p>

    <h3>pyLinkedCMD</h3>
    <p class="mb-0">The <a href="https://github.com/skybristol/pylinkedcmd">pyLinkedCMD</a> package is a set of Python codes for operating against all of the various data sources that we are processing. At this point, everything we are dealing with is completely in the public domain on readily accessible online resources with a variety of APIs or other interfaces. pyLinkedCMD classes are developed for handling the interfaces we need to make and processing into our abstract data model.</p>

</blockquote>
{% endblock %}